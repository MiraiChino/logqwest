import duckdb
import pandas as pd
from pathlib import Path
import json

from config import CHECK_RESULT_DIR, DATA_DIR, LOGCHECK_HEADERS, ADVCHECK_HEADERS, LOCATIONCHECK_HEADERS

ADVENTURE_DETAIL_LINE_THRESHOLD = 160  # å†’é™ºè©³ç´°ãƒ•ã‚¡ã‚¤ãƒ«ã®å®Œäº†è¡Œæ•°é–¾å€¤
CHECK_MARK = "âœ…"
SUCCESS_EMOJI = "â—"

def get_check_log_csv_path(area: str) -> Path:
    """æŒ‡å®šã•ã‚ŒãŸã‚¨ãƒªã‚¢åã®ãƒ­ã‚°ãƒã‚§ãƒƒã‚¯CSVãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’è¿”ã™ã€‚"""
    return CHECK_RESULT_DIR / area / f"log_{area}.csv"

def get_check_adv_csv_path(area: str) -> Path:
    """æŒ‡å®šã•ã‚ŒãŸã‚¨ãƒªã‚¢åã®å†’é™ºãƒã‚§ãƒƒã‚¯CSVãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’è¿”ã™ã€‚"""
    return CHECK_RESULT_DIR / area / f"adv_{area}.csv"

def get_check_loc_csv_path(area: str) -> Path:
    """æŒ‡å®šã•ã‚ŒãŸã‚¨ãƒªã‚¢åã®ä½ç½®æƒ…å ±ãƒã‚§ãƒƒã‚¯CSVãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’è¿”ã™ã€‚"""
    return CHECK_RESULT_DIR / area / f"loc_{area}.csv"

def get_data_path() -> Path:
    return DATA_DIR

def get_area_csv_path(area: str) -> Path:
    """æŒ‡å®šã•ã‚ŒãŸã‚¨ãƒªã‚¢åã®CSVãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’è¿”ã™ã€‚"""
    return DATA_DIR / area / f"{area}.csv"

def get_adventure_path(area: str, adv: str) -> Path:
    """æŒ‡å®šã•ã‚ŒãŸã‚¨ãƒªã‚¢å†…ã®å†’é™ºãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’è¿”ã™ã€‚"""
    return DATA_DIR / area / f"{adv}.txt"

def get_location_path(area: str, adv: str) -> Path:
    """æŒ‡å®šã•ã‚ŒãŸã‚¨ãƒªã‚¢å†…ã®å†’é™ºãƒ†ã‚­ã‚¹ãƒˆã®ä½ç½®ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’è¿”ã™ã€‚"""
    return DATA_DIR / area / f"loc_{adv}.txt"

def is_adventure_complete(area: str, adventure_name: str) -> bool:
    """å†’é™ºè©³ç´°ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã€å†…å®¹ãŒæŒ‡å®šè¡Œæ•°ä»¥ä¸Šã®å ´åˆã«Trueã‚’è¿”ã™ã€‚"""
    adventure_path = get_adventure_path(area, adventure_name)
    if not adventure_path.exists():
        return False
    try:
        with open(adventure_path, "r", encoding="utf-8") as f:
            lines = f.readlines()
        return len(lines) >= ADVENTURE_DETAIL_LINE_THRESHOLD
    except Exception:
        return False

def is_area_complete(area: str) -> bool:
    """
    ã‚¨ãƒªã‚¢å†…ã®å…¨ã¦ã®å†’é™ºãŒå®Œäº†ã—ã€ã‹ã¤ãƒã‚§ãƒƒã‚¯çµæœCSVã®è¡Œæ•°ã‚‚ä¸€è‡´ã™ã‚‹ã‹åˆ¤å®šã™ã‚‹ã€‚
    å®Œäº†ã¨ã¯ã€å†’é™ºè©³ç´°ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã€å†…å®¹ãŒæŒ‡å®šè¡Œæ•°ä»¥ä¸Šã§ã‚ã‚‹ã“ã¨ã€‚
    """
    area_csv_path = get_area_csv_path(area)
    df_area = load_csv(area_csv_path)
    check_log_csv_path = get_check_log_csv_path(area)
    df_check_logs = load_csv(check_log_csv_path)
    check_adv_csv_path = get_check_adv_csv_path(area)
    df_check_advs = load_csv(check_adv_csv_path)
    check_loc_csv_path = get_check_loc_csv_path(area)
    df_check_locs = load_csv(check_loc_csv_path)

    if (df_area is None or "å†’é™ºå" not in df_area.columns or df_area.empty):
        return False

    total_adventures = len(df_area)
    completed_adventures_count = sum(1 for adv in df_area["å†’é™ºå"] if is_adventure_complete(area, adv))
    checked_logs_count = len(df_check_logs) if df_check_logs is not None else 0 # ãƒã‚§ãƒƒã‚¯çµæœCSVãŒãªã„å ´åˆã¯0
    checked_advs_count = len(df_check_advs) if df_check_advs is not None else 0 # ãƒã‚§ãƒƒã‚¯çµæœCSVãŒãªã„å ´åˆã¯0
    checked_locs_count = len(df_check_locs) if df_check_locs is not None else 0 # ãƒã‚§ãƒƒã‚¯çµæœCSVãŒãªã„å ´åˆã¯0

    return total_adventures == completed_adventures_count == checked_logs_count == checked_advs_count == checked_locs_count

def is_area_all_checked(area: str) -> bool:
    """
    ã‚¨ãƒªã‚¢ã® check_logs_csv, check_advs_csv ã®ãƒã‚§ãƒƒã‚¯å†…å®¹ãŒå…¨ã¦âœ…ã§å§‹ã¾ã£ã¦ã„ã‚‹ã‹ç¢ºèªã™ã‚‹ã€‚
    å†’é™ºã®å®Œäº†çŠ¶æ…‹ã‚„ãƒã‚§ãƒƒã‚¯çµæœCSVã®è¡Œæ•°ã¯è¦‹ãªã„ã€‚
    """
    check_log_csv_path = get_check_log_csv_path(area)
    df_check_logs = load_csv(check_log_csv_path)

    if df_check_logs is None or df_check_logs.empty:
        return False
    
    check_columns = LOGCHECK_HEADERS[1:-1] # 'å†’é™ºå', 'ç·åˆè©•ä¾¡' ä»¥å¤–ã®åˆ—ã‚’ãƒã‚§ãƒƒã‚¯
    for _, row in df_check_logs.iterrows():
        for col in check_columns:
            if not isinstance(row[col], str) or not row[col].startswith(CHECK_MARK):
                return False

    check_adv_csv_path = get_check_adv_csv_path(area)
    df_check_advs = load_csv(check_adv_csv_path)

    if df_check_advs is None or df_check_advs.empty:
        return False
    
    check_columns = ADVCHECK_HEADERS[1:-1] # 'å†’é™ºå', 'ç·åˆè©•ä¾¡' ä»¥å¤–ã®åˆ—ã‚’ãƒã‚§ãƒƒã‚¯
    for _, row in df_check_advs.iterrows():
        for col in check_columns:
            if not isinstance(row[col], str) or not row[col].startswith(CHECK_MARK):
                return False

    check_loc_csv_path = get_check_loc_csv_path(area)
    df_check_locs = load_csv(check_loc_csv_path)

    if df_check_locs is None or df_check_locs.empty:
        return False

    check_columns = LOCATIONCHECK_HEADERS[1:-1] # 'å†’é™ºå', 'ç·åˆè©•ä¾¡' ä»¥å¤–ã®åˆ—ã‚’ãƒã‚§ãƒƒã‚¯
    for _, row in df_check_locs.iterrows():
        for col in check_columns:
            if col not in row or not isinstance(row[col], str) or not row[col].startswith(CHECK_MARK): # åˆ—ãŒå­˜åœ¨ã—ãªã„å ´åˆã‚‚è€ƒæ…®
                return False
    return True # å…¨ã¦ã®é …ç›®ãŒâœ…ã§å§‹ã¾ã£ã¦ã„ã‚Œã°Trueã‚’è¿”ã™

def delete_locations(area: str, advs_to_delete: list):
    """
    æŒ‡å®šã•ã‚ŒãŸ area ã«å¯¾ã—ã¦ã€
    ãƒ»get_check_loc_csv_path ã®ã€Œå†’é™ºåã€åˆ—ã«è©²å½“ã™ã‚‹è¡Œã‚’å‰Šé™¤
    ãƒ»get_location_path ã®ã€Œå†’é™ºåã€åˆ—ã«è©²å½“ã™ã‚‹è¡Œã‚’å‰Šé™¤
    """
    # get_check_adv_csv_path ã®å‡¦ç†
    check_results_path = Path(get_check_loc_csv_path(area))
    if check_results_path.exists():
        df_check = pd.read_csv(check_results_path)
        if "å†’é™ºå" in df_check.columns:
            df_check = df_check[~df_check["å†’é™ºå"].isin(advs_to_delete)]
            df_check.to_csv(check_results_path, index=False)
            yield f"ğŸ”¥ å‰Šé™¤: {advs_to_delete} from {check_results_path}"
        else:
            yield f"â— ã‚¨ãƒ©ãƒ¼: {check_results_path} ã« 'å†’é™ºå' åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
    else:
        yield f"âŒ è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {check_results_path}"

    # get_location_path ã®å‡¦ç†
    for adv in advs_to_delete:
        txt_path = Path(get_location_path(area, adv))
        if txt_path.exists():
            txt_path.unlink()
            yield f"ğŸ”¥ å‰Šé™¤: {txt_path}"
        else:
            yield f"âŒ è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {txt_path}"

def delete_adventures(area: str, advs_to_delete: list):
    """
    æŒ‡å®šã•ã‚ŒãŸ area ã«å¯¾ã—ã¦ã€
    ãƒ»get_check_adv_csv_path ã®ã€Œå†’é™ºåã€åˆ—ã«è©²å½“ã™ã‚‹è¡Œã‚’å‰Šé™¤
    ãƒ»get_area_csv_path ã®ã€Œå†’é™ºåã€åˆ—ã«è©²å½“ã™ã‚‹è¡Œã‚’å‰Šé™¤
    """
    # get_check_adv_csv_path ã®å‡¦ç†
    check_results_path = Path(get_check_adv_csv_path(area))
    if check_results_path.exists():
        df_check = pd.read_csv(check_results_path)
        if "å†’é™ºå" in df_check.columns:
            df_check = df_check[~df_check["å†’é™ºå"].isin(advs_to_delete)]
            df_check.to_csv(check_results_path, index=False)
            yield f"ğŸ”¥ å‰Šé™¤: {advs_to_delete} from {check_results_path}"
        else:
            yield f"â— ã‚¨ãƒ©ãƒ¼: {check_results_path} ã« 'å†’é™ºå' åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
    else:
        yield f"âŒ è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {check_results_path}"

    # get_area_csv_path ã®å‡¦ç†
    area_csv_path = Path(get_area_csv_path(area))
    if area_csv_path.exists():
        df_area = pd.read_csv(area_csv_path)
        if "å†’é™ºå" in df_area.columns:
            df_area = df_area[~df_area["å†’é™ºå"].isin(advs_to_delete)]
            df_area.to_csv(area_csv_path, index=False)
            yield f"ğŸ”¥ å‰Šé™¤: {advs_to_delete} from {area_csv_path}"
        else:
            yield f"â— ã‚¨ãƒ©ãƒ¼: {area_csv_path} ã« 'å†’é™ºå' åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
    else:
        yield f"âŒ è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {area_csv_path}"

    yield from delete_logs(area, advs_to_delete)


def delete_logs(area: str, advs_to_delete: list):
    """
    æŒ‡å®šã•ã‚ŒãŸ area ã«å¯¾ã—ã¦ã€
    ãƒ»get_check_log_csv_path ã®ã€Œå†’é™ºåã€åˆ—ã«è©²å½“ã™ã‚‹è¡Œã‚’å‰Šé™¤
    ãƒ»get_adventure_path ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
    ãƒ»
    """
    # get_check_log_csv_path ã®å‡¦ç†
    check_results_path = Path(get_check_log_csv_path(area))
    if check_results_path.exists():
        df_check = pd.read_csv(check_results_path)
        df_check = df_check[~df_check["å†’é™ºå"].isin(advs_to_delete)]
        df_check.to_csv(check_results_path, index=False)
        yield f"ğŸ”¥ å‰Šé™¤: {advs_to_delete} from {check_results_path}"
    else:
        yield f"âŒ è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {check_results_path}"
    
    # get_adventure_path ã®å‡¦ç†
    for adv in advs_to_delete:
        txt_path = Path(get_adventure_path(area, adv))
        if txt_path.exists():
            txt_path.unlink()
            yield f"ğŸ”¥ å‰Šé™¤: {txt_path}"
        else:
            yield f"âŒ è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {txt_path}"

    yield from delete_locations(area, advs_to_delete)
    
def get_outcome_emoji(outcome: str) -> str:
    """çµæœã«å¿œã˜ã¦çµµæ–‡å­—ã‚’è¿”ã™"""
    outcome_emojis = {
        "å¤§æˆåŠŸ": "ğŸ’",
        "æˆåŠŸ": "ğŸ",
        "å¤±æ•—": "âŒ",
    }
    return outcome_emojis.get(outcome, "")

def load_csv(csv_path: Path) -> pd.DataFrame:
    """
    DuckDBã‚’ä½¿ã£ã¦SQLã‚¯ã‚¨ãƒªã§CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿DataFrameã‚’è¿”ã™ã€‚
    ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ None ã‚’è¿”ã™ã€‚
    """
    if not csv_path.exists():
        return None
    try:
        con = duckdb.connect(database=':memory:', read_only=False)
        sql_query = f"SELECT * FROM read_csv_auto('{str(csv_path)}', header = true)"
        df = con.execute(sql_query).fetchdf()
        con.close()
        return df
    except Exception as e:
        print(f"DuckDBã§CSVèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return None

USER_DATA_FILE = Path("user_data") / "history.json"

def load_usage_data(user_data_file: Path = USER_DATA_FILE) -> dict:
    """
    ä½¿ç”¨å±¥æ­´ã¨åæ”¯æƒ…å ±ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã‚€ã€‚
    """
    try:
        with user_data_file.open("r", encoding="utf-8") as f:
            data = json.load(f)
            if "adventure_history" not in data: # åˆå›èµ·å‹•æ™‚ãªã©ã§ã‚­ãƒ¼ãŒå­˜åœ¨ã—ãªã„å ´åˆ
                return {"adventure_history": []} # ç©ºã®ãƒªã‚¹ãƒˆã§åˆæœŸåŒ–
            data["adventure_history"].sort(key=lambda item: item["timestamp"], reverse=True)
            return data
    except (FileNotFoundError, json.JSONDecodeError):
        return {"adventure_history": []}

def save_usage_data(data: dict, user_data_file: Path = USER_DATA_FILE):
    """ä½¿ç”¨å±¥æ­´ã¨åæ”¯æƒ…å ±ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã¸ä¿å­˜ã™ã‚‹ã€‚"""
    user_data_file.parent.mkdir(parents=True, exist_ok=True)
    with user_data_file.open("w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)